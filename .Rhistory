setwd("~/GitHub/niconico_analysis")
require(e1071)
require(randomForest)
require(ggplot2)
require(knitr)
# 予め作成したノードリストを読み込む
set<-read.csv("taggraph_nodes.csv",fileEncoding = "CP932")
set<-na.omit(set)
#再生数などの情報は既にポイントで反映していると考え削除しておく。計算結果を最終的にsetに貼って出力しGephiでまた使う
set1<-set[,c(-2,-3,-10,-17,-18,-19)]
plus<-subset(set1,set1$Points>2.5*mean(set1$Points))
minus<-subset(set1,set1$Points<0.05*mean(set1$Points))
plus$culture<-as.factor(1)
minus$culture<-as.factor(0)
# チューニング用とテスト用に分ける
plus1<-plus[c(1:47),]
plus2<-plus[c(48:94),]
minus1<-minus[c(1:46),]
minus2<-minus[c(47:92),]
learning<-data.frame(rbind(plus1,minus1))
test<-data.frame(rbind(plus2,minus2))
# learningは等しい数にしたい
learning<-data.frame(rbind(learning,test[93,]))
test<-test[-93,]
# 日付型は使えないのでnumericに。あと、Hubは削除。
set1$ave_starttime<-as.numeric(set1$ave_starttime)
set1$Authority<-as.numeric(set1$Authority)
learning1<-learning[,c(-1,-5,-16)]
learning1$ave_starttime<-as.numeric(learning1$ave_starttime)
learning1$Authority<-as.numeric(learning1$Authority)
test1<-test[,c(-1,-5,-16)]
test1$ave_starttime<-as.numeric(test1$ave_starttime)
test1$Authority<-as.numeric(test1$Authority)
# 最適なところで重要度を計算したい
tuneRF(learning1[,-14],learning1[,14],doBest=T)
# mtry=12が最適な模様
# 文字化けに気をつけて。。。UTF-8にして日本語を打ち込むと良い模様
importance<-randomForest(culture~.,learning1,mtry=12,importance = TRUE)
knitr::kable(importance(importance))
imp<-importance(importance)
imp<-data.frame(imp)
View(imp)
imp<-imp[,c(-1,-2,-3)]
imp<-data.frame(imp)
imp1<-importance(importance)
imp1<-data.frame(imp1)
names(imp)<-"MeanDecreaseGini"
imp$variable<-row.names(imp1)
g <- ggplot(
imp,                    # ggplot 用データフレーム
aes (                  # ggplot オプション設定
x = variable,           # x 軸を df$group とする
y = MeanDecreaseGini,          # y 軸を df$length とする
fill = variable        # df$group に従ってグループ分ける
)
)
g <- g + geom_bar(                    # plotbarに当たる関数
width = 0.8,                        # 棒の幅
stat = "identity"
)
g <- g + xlab("特徴量")            # x 軸名
g <- g + ylab("重要度")       # y 軸名
g <- g + ggtitle("特徴量毎の重要度比較")
g<-g + coord_flip()
plot(g)
imp[1,2]<-"Degree"
require(ggplot2)
nodes<-read.csv("gephinodesver2.csv")
p1 <- ggplot(nodes, aes(x=probability, y=次数))
p1 + geom_point(aes(colour=probability))
p2 <- ggplot(nodes, aes(x=probability, y=Authority))
p2 + geom_point(aes(colour=probability))
p3 <- ggplot(nodes, aes(x=probability, y=Modularity.Class))
p3 + geom_point(aes(colour=Points))
p4 <- ggplot(nodes, aes(x=probability, y=PageRank))
p4 + geom_point(aes(colour=probability))
p5 <- ggplot(nodes, aes(x=probability, y=Clustering.Coefficient))
p5 + geom_point(aes(colour=probability))
p6 <- ggplot(nodes, aes(x=probability, y=Number.of.triangles))
p6 + geom_point(aes(colour=probability))
p7 <- ggplot(nodes, aes(x=probability, y=Eigenvector.Centrality))
p7 + geom_point(aes(colour=probability))
p8 <- ggplot(nodes, aes(x=probability, y=ave_view))
p8 + geom_point(aes(colour=probability))
p9 <- ggplot(nodes, aes(x=probability, y=ave_comment))
p9 + geom_point(aes(colour=probability))
p10 <- ggplot(nodes, aes(x=probability, y=ave_mylist))
p10 + geom_point(aes(colour=probability))
p11 <- ggplot(nodes, aes(x=probability, y=Points))
p11 + geom_point(aes(colour=probability)) + geom_smooth(method="lm")
p12 <- ggplot(nodes, aes(x=probability, y=ave_length))
p12 + geom_point(aes(colour=probability)) + geom_smooth(method="lm")
p13 <- ggplot(nodes, aes(x=probability, y=ave_starttime))
p13 + geom_point(aes(colour=probability))
View(nodes)
nodes<-read.csv("taggraph_nodes.csv")
nodes<-read.csv("taggraph_nodes.csv",fileEncoding="cp932")
View(nodes)
nodes<-read.csv("taggraph(Nodes)mmmver2.csv",fileEncoding="cp932")
nodes<-read.csv("taggraph（Nodes)mmmver2.csv",fileEncoding="cp932")
nodes<-read.csv("taggraph (Nodes)mmmver2.csv",fileEncoding="cp932")
View(nodes)
nodes$probability<-nodes$modify_probability
require(ggplot2)
nodes<-read.csv("gephinodesver2.csv")
p1 <- ggplot(nodes, aes(x=probability, y=次数))
p1 + geom_point(aes(colour=probability))
p2 <- ggplot(nodes, aes(x=probability, y=Authority))
p2 + geom_point(aes(colour=probability))
p3 <- ggplot(nodes, aes(x=probability, y=Modularity.Class))
p3 + geom_point(aes(colour=Points))
p4 <- ggplot(nodes, aes(x=probability, y=PageRank))
p4 + geom_point(aes(colour=probability))
p5 <- ggplot(nodes, aes(x=probability, y=Clustering.Coefficient))
p5 + geom_point(aes(colour=probability))
p6 <- ggplot(nodes, aes(x=probability, y=Number.of.triangles))
p6 + geom_point(aes(colour=probability))
p7 <- ggplot(nodes, aes(x=probability, y=Eigenvector.Centrality))
p7 + geom_point(aes(colour=probability))
p8 <- ggplot(nodes, aes(x=probability, y=ave_view))
p8 + geom_point(aes(colour=probability))
p9 <- ggplot(nodes, aes(x=probability, y=ave_comment))
p9 + geom_point(aes(colour=probability))
p10 <- ggplot(nodes, aes(x=probability, y=ave_mylist))
p10 + geom_point(aes(colour=probability))
p11 <- ggplot(nodes, aes(x=probability, y=Points))
p11 + geom_point(aes(colour=probability)) + geom_smooth(method="lm")
p12 <- ggplot(nodes, aes(x=probability, y=ave_length))
p12 + geom_point(aes(colour=probability)) + geom_smooth(method="lm")
p13 <- ggplot(nodes, aes(x=probability, y=ave_starttime))
p13 + geom_point(aes(colour=probability))
p10 <- ggplot(nodes, aes(x=probability, y=ave_mylist)) + geom_smooth(method="lm") + geom_point(aes(colour=probability))
plot(p10)
p8 <- ggplot(nodes, aes(x=probability, y=ave_view)) + geom_smooth(method="lm") + geom_point(aes(colour=probability))
plot(p8)
p9 <- ggplot(nodes, aes(x=probability, y=ave_comment)) + geom_smooth(method="lm") + geom_point(aes(colour=probability))
plot(p9)
